{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61eb3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import holidays\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0646f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    FINAL CORRECTED version using merge operations to avoid index issues\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import holidays\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Ensure datetime is properly parsed\n",
    "    if data['datetime'].dtype == 'object':\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "    \n",
    "    # Create incident_datetime if it doesn't exist\n",
    "    if 'incident_datetime' not in data.columns:\n",
    "        data['incident_datetime'] = data['datetime']\n",
    "    \n",
    "    # Add row identifier to preserve order\n",
    "    data['_row_id'] = range(len(data))\n",
    "    \n",
    "    # ===== TEMPORAL FEATURES =====\n",
    "    \n",
    "    # Basic time components\n",
    "    data['hour'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "    data['quarter'] = data['datetime'].dt.quarter\n",
    "    data['day_of_year'] = data['datetime'].dt.dayofyear\n",
    "    \n",
    "    # Binary temporal indicators\n",
    "    data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)  # Saturday, Sunday\n",
    "    data['is_night'] = ((data['hour'] >= 22) | (data['hour'] <= 6)).astype(int)  # 10 PM - 6 AM\n",
    "    data['is_rush_hour'] = (((data['hour'] >= 7) & (data['hour'] <= 9)) | \n",
    "                           ((data['hour'] >= 16) & (data['hour'] <= 18))).astype(int)  # Morning & evening rush\n",
    "    data['is_business_hours'] = ((data['hour'] >= 9) & (data['hour'] <= 17) & \n",
    "                                (data['day_of_week'] < 5)).astype(int)  # 9 AM - 5 PM weekdays\n",
    "    \n",
    "    # Season mapping\n",
    "    data['season'] = data['month'].map({\n",
    "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "        9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "    })\n",
    "    \n",
    "    # Shift mapping using a proper approach\n",
    "    def get_shift(hour):\n",
    "        if 6 <= hour < 14:\n",
    "            return 'Day'\n",
    "        elif 14 <= hour < 22:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'\n",
    "    \n",
    "    data['shift'] = data['hour'].apply(get_shift)\n",
    "    \n",
    "    # Holiday indicator (US federal holidays)\n",
    "    us_holidays = holidays.US()\n",
    "    data['is_holiday'] = data['datetime'].dt.date.isin(us_holidays).astype(int)\n",
    "    \n",
    "    # ===== GEOGRAPHIC FEATURES =====\n",
    "    \n",
    "    # Distance from center (assuming Nashville city center: 36.1627, -86.7816)\n",
    "    nashville_center_lat = 36.1627\n",
    "    nashville_center_lon = -86.7816\n",
    "    \n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate the great circle distance between two points on earth\"\"\"\n",
    "        R = 3959  # Earth's radius in miles\n",
    "        \n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        distance = R * c\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    data['distance_from_center'] = data.apply(lambda row: haversine_distance(\n",
    "        row['lat'], row['lon'], nashville_center_lat, nashville_center_lon), axis=1)\n",
    "    \n",
    "    # ===== WORKLOAD FEATURES USING MERGE APPROACH =====\n",
    "    \n",
    "    # Sort by datetime for all calculations\n",
    "    data = data.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    print(\"Calculating workload features using merge approach...\")\n",
    "    \n",
    "    # 1. System-wide incidents in last hour\n",
    "    data_temp = data.set_index('datetime')\n",
    "    data_temp['count_helper'] = 1\n",
    "    system_workload = data_temp['count_helper'].rolling('1h', closed='left').sum().fillna(0)\n",
    "    data['system_incidents_last_hour'] = system_workload.values\n",
    "    \n",
    "    # 2. Category-specific incidents in last 24 hours using merge approach\n",
    "    if 'category' in data.columns:\n",
    "        print(\"Calculating category-specific workload...\")\n",
    "        category_workload_list = []\n",
    "        \n",
    "        for category in data['category'].unique():\n",
    "            if pd.isna(category):\n",
    "                continue\n",
    "            \n",
    "            # Get data for this category only\n",
    "            cat_data = data[data['category'] == category][['datetime', '_row_id']].copy()\n",
    "            cat_data = cat_data.set_index('datetime')\n",
    "            cat_data['count_helper'] = 1\n",
    "            \n",
    "            # Calculate rolling sum for this category\n",
    "            cat_rolling = cat_data['count_helper'].rolling('24h', closed='left').sum().fillna(0)\n",
    "            \n",
    "            # Create dataframe with results\n",
    "            cat_results = pd.DataFrame({\n",
    "                '_row_id': cat_data['_row_id'],\n",
    "                'category_incidents_last_24h': cat_rolling.values\n",
    "            })\n",
    "            \n",
    "            category_workload_list.append(cat_results)\n",
    "        \n",
    "        # Combine all category results\n",
    "        if category_workload_list:\n",
    "            all_category_workload = pd.concat(category_workload_list, ignore_index=True)\n",
    "            # Merge back to main data\n",
    "            data = data.merge(all_category_workload, on='_row_id', how='left')\n",
    "            data['category_incidents_last_24h'] = data['category_incidents_last_24h'].fillna(0)\n",
    "        else:\n",
    "            data['category_incidents_last_24h'] = 0\n",
    "    else:\n",
    "        data['category_incidents_last_24h'] = 0\n",
    "    \n",
    "    # 3. Zone-specific incidents using merge approach\n",
    "    if 'ZONE_ID' in data.columns:\n",
    "        print(\"Calculating zone-specific workload...\")\n",
    "        zone_workload_list = []\n",
    "        \n",
    "        for zone_id in data['ZONE_ID'].unique():\n",
    "            if pd.isna(zone_id):\n",
    "                continue\n",
    "            \n",
    "            # Get data for this zone only\n",
    "            zone_data = data[data['ZONE_ID'] == zone_id][['datetime', '_row_id']].copy()\n",
    "            zone_data = zone_data.set_index('datetime')\n",
    "            zone_data['count_helper'] = 1\n",
    "            \n",
    "            # Calculate rolling sums for this zone\n",
    "            zone_rolling_week = zone_data['count_helper'].rolling('7D', closed='left').sum().fillna(0)\n",
    "            zone_rolling_month = zone_data['count_helper'].rolling('30D', closed='left').sum().fillna(0)\n",
    "            \n",
    "            # Create dataframe with results\n",
    "            zone_results = pd.DataFrame({\n",
    "                '_row_id': zone_data['_row_id'],\n",
    "                'zone_incidents_last_week': zone_rolling_week.values,\n",
    "                'zone_incidents_last_month': zone_rolling_month.values\n",
    "            })\n",
    "            \n",
    "            zone_workload_list.append(zone_results)\n",
    "        \n",
    "        # Combine all zone results\n",
    "        if zone_workload_list:\n",
    "            all_zone_workload = pd.concat(zone_workload_list, ignore_index=True)\n",
    "            # Merge back to main data\n",
    "            data = data.merge(all_zone_workload, on='_row_id', how='left')\n",
    "            data['zone_incidents_last_week'] = data['zone_incidents_last_week'].fillna(0)\n",
    "            data['zone_incidents_last_month'] = data['zone_incidents_last_month'].fillna(0)\n",
    "        else:\n",
    "            data['zone_incidents_last_week'] = 0\n",
    "            data['zone_incidents_last_month'] = 0\n",
    "    else:\n",
    "        data['zone_incidents_last_week'] = 0\n",
    "        data['zone_incidents_last_month'] = 0\n",
    "    \n",
    "    # Remove helper column\n",
    "    data = data.drop(columns=['_row_id'])\n",
    "    \n",
    "    print(\"Workload feature calculation completed!\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca20663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FireIncidentResponsePredictor:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        self.preprocessor = None\n",
    "        self.feature_columns = []\n",
    "        self.is_trained = False\n",
    "        self.training_results = {}\n",
    "        \n",
    "    def _prepare_features(self, data, target_column='response_time'):\n",
    "        \"\"\"Prepare features from data - used internally\"\"\"\n",
    "        # Apply feature engineering if not already done\n",
    "        if 'incident_datetime' not in data.columns:\n",
    "            data = engineer_features(data.copy())\n",
    "        \n",
    "        # Define feature groups\n",
    "        temporal_features = [\n",
    "            'hour', 'day_of_week', 'month', 'quarter', 'day_of_year',\n",
    "            'is_weekend', 'is_night', 'is_rush_hour', 'is_business_hours', 'is_holiday'\n",
    "        ]\n",
    "        \n",
    "        workload_features = [\n",
    "            'category_incidents_last_24h', 'system_incidents_last_hour'\n",
    "        ]\n",
    "        \n",
    "        geographic_features = [\n",
    "            'lat', 'lon', 'distance_from_center', 'zone_incidents_last_week', 'zone_incidents_last_month'\n",
    "        ]\n",
    "        \n",
    "        categorical_features = [\n",
    "            'shift', 'season', 'category', 'ZONE_ID', 'incident_type'\n",
    "        ]\n",
    "        \n",
    "        # Combine all numerical features\n",
    "        numerical_features = temporal_features + workload_features + geographic_features\n",
    "        \n",
    "        # Filter features that exist in the data\n",
    "        available_numerical = [f for f in numerical_features if f in data.columns]\n",
    "        available_categorical = [f for f in categorical_features if f in data.columns]\n",
    "        \n",
    "        # Store feature columns for later use\n",
    "        feature_columns = available_numerical + available_categorical\n",
    "        \n",
    "        # Filter valid data\n",
    "        if target_column in data.columns:\n",
    "            valid_mask = (\n",
    "                data[target_column].notna() & \n",
    "                (data[target_column] > 0)\n",
    "            )\n",
    "            df_clean = data[valid_mask].copy()\n",
    "        else:\n",
    "            df_clean = data.copy()\n",
    "        \n",
    "        return df_clean, feature_columns, available_numerical, available_categorical\n",
    "    \n",
    "    def train(self, train_data, target_column='response_time_seconds', test_size=0.15, val_size=0.15):\n",
    "        \"\"\"Train all models on the provided dataset\"\"\"\n",
    "        \n",
    "        print(\"Fire Incident Response Time Prediction - Training\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Prepare training data\n",
    "        df_clean, feature_columns, available_numerical, available_categorical = self._prepare_features(\n",
    "            train_data, target_column\n",
    "        )\n",
    "        \n",
    "        self.feature_columns = feature_columns\n",
    "        \n",
    "        print(f\"Available numerical features: {len(available_numerical)}\")\n",
    "        print(f\"Available categorical features: {len(available_categorical)}\")\n",
    "        print(f\"Valid training samples: {len(df_clean)}\")\n",
    "        \n",
    "        X = df_clean[feature_columns]\n",
    "        y = df_clean[target_column]\n",
    "        \n",
    "        # Create preprocessor\n",
    "        numerical_transformer = StandardScaler()\n",
    "        categorical_transformer = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, available_numerical),\n",
    "                ('cat', categorical_transformer, available_categorical)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Initialize models\n",
    "        self._initialize_models()\n",
    "        \n",
    "        # Split data (temporal split if datetime available)\n",
    "        if 'incident_datetime' in df_clean.columns:\n",
    "            sort_idx = df_clean['incident_datetime'].argsort()\n",
    "            sorted_X = X.iloc[sort_idx]\n",
    "            sorted_y = y.iloc[sort_idx]\n",
    "            \n",
    "            n_samples = len(sorted_X)\n",
    "            train_end = int(n_samples * (1 - test_size - val_size))\n",
    "            val_end = int(n_samples * (1 - test_size))\n",
    "            \n",
    "            X_train = sorted_X.iloc[:train_end]\n",
    "            X_val = sorted_X.iloc[train_end:val_end]\n",
    "            X_test = sorted_X.iloc[val_end:]\n",
    "            \n",
    "            y_train = sorted_y.iloc[:train_end]\n",
    "            y_val = sorted_y.iloc[train_end:val_end]\n",
    "            y_test = sorted_y.iloc[val_end:]\n",
    "        else:\n",
    "            X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=42\n",
    "            )\n",
    "            val_size_adjusted = val_size / (1 - test_size)\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_temp, y_temp, test_size=val_size_adjusted, random_state=42\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nTraining set: {len(X_train)}\")\n",
    "        print(f\"Validation set: {len(X_val)}\")\n",
    "        print(f\"Test set: {len(X_test)}\")\n",
    "        \n",
    "        # Train all models\n",
    "        self.training_results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            # Create pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('regressor', model)\n",
    "            ])\n",
    "            \n",
    "            # Hyperparameter tuning\n",
    "            if name in self.param_grids:\n",
    "                grid_search = GridSearchCV(\n",
    "                    pipeline, self.param_grids[name], \n",
    "                    cv=2, scoring='neg_mean_absolute_error', n_jobs=-1\n",
    "                )\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_pipeline = grid_search.best_estimator_\n",
    "                print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "            else:\n",
    "                best_pipeline = pipeline\n",
    "                best_pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Store trained model\n",
    "            self.models[name] = best_pipeline\n",
    "            \n",
    "            # Evaluate\n",
    "            y_val_pred = best_pipeline.predict(X_val)\n",
    "            y_test_pred = best_pipeline.predict(X_test)\n",
    "            \n",
    "            # Metrics\n",
    "            val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "            \n",
    "            self.training_results[name] = {\n",
    "                'val_mae': val_mae,\n",
    "                'test_mae': test_mae,\n",
    "                'test_rmse': test_rmse,\n",
    "                'test_r2': test_r2\n",
    "            }\n",
    "            \n",
    "            print(f\"Validation MAE: {val_mae:.2f}, Test MAE: {test_mae:.2f}, Test R²: {test_r2:.3f}\")\n",
    "        \n",
    "        # Find best model\n",
    "        self.best_model_name = min(self.training_results.keys(), \n",
    "                                 key=lambda x: self.training_results[x]['test_mae'])\n",
    "        self.best_model = self.models[self.best_model_name]\n",
    "        self.is_trained = True\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"TRAINING COMPLETED\")\n",
    "        print(f\"Best Model: {self.best_model_name}\")\n",
    "        print(f\"Best Test MAE: {self.training_results[self.best_model_name]['test_mae']:.2f}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.training_results\n",
    "    \n",
    "    def test(self, test_data, target_column='response_time_seconds', models_to_test=None):\n",
    "        \"\"\"Test trained models on new dataset\"\"\"\n",
    "        \n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Models not trained yet. Call train() first.\")\n",
    "        \n",
    "        print(f\"\\nTesting models on new dataset...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Prepare test data\n",
    "        df_clean, _, _, _ = self._prepare_features(test_data, target_column)\n",
    "        \n",
    "        # Use same feature columns as training\n",
    "        available_features = [f for f in self.feature_columns if f in df_clean.columns]\n",
    "        missing_features = set(self.feature_columns) - set(available_features)\n",
    "        \n",
    "        if missing_features:\n",
    "            print(f\"Warning: Missing features in test data: {missing_features}\")\n",
    "        \n",
    "        X_test = df_clean[available_features]\n",
    "        y_test = df_clean[target_column] if target_column in df_clean.columns else None\n",
    "        \n",
    "        print(f\"Test samples: {len(X_test)}\")\n",
    "        \n",
    "        # Test specified models or all models\n",
    "        models_to_evaluate = models_to_test or list(self.models.keys())\n",
    "        test_results = {}\n",
    "        \n",
    "        for model_name in models_to_evaluate:\n",
    "            if model_name not in self.models:\n",
    "                print(f\"Warning: Model '{model_name}' not found\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                model = self.models[model_name]\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                result = {\n",
    "                    'predictions': y_pred,\n",
    "                    'model': model\n",
    "                }\n",
    "                \n",
    "                # Calculate metrics if target is available\n",
    "                if y_test is not None:\n",
    "                    mae = mean_absolute_error(y_test, y_pred)\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "                    mse = mean_squared_error(y_test, y_pred)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    \n",
    "                    # Bias (mean error)\n",
    "                    bias = np.mean(y_pred - y_test)\n",
    "                    \n",
    "                    # Variance of predictions\n",
    "                    variance = np.var(y_pred)\n",
    "                    \n",
    "                    # Additional metrics\n",
    "                    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "                    r2 = 1 - (np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))  # R-squared\n",
    "                    result.update({\n",
    "                        'mae': mae,\n",
    "                        'rmse': rmse,\n",
    "                        'r2': r2,\n",
    "                        'actual': y_test,\n",
    "                        'mape': mape,\n",
    "                        'bias': bias,\n",
    "                        'variance': variance\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"{model_name:20} - MAE: {mae:6.2f}, RMSE: {rmse:6.2f}, R²: {r2:6.3f}, MAPE: {mape:6.2f}%, Bias: {bias:6.2f}, Variance: {variance:6.2f}\")\n",
    "                else:\n",
    "                    print(f\"{model_name:20} - Predictions generated (no target for evaluation)\")\n",
    "                \n",
    "                test_results[model_name] = result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error testing {model_name}: {e}\")\n",
    "        \n",
    "        return test_results\n",
    "    \n",
    "    def predict(self, new_data, model_name=None, feature_columns=None):\n",
    "        \"\"\"Make predictions on new data using specified model or best model\"\"\"\n",
    "        \n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Models not trained yet. Call train() first.\")\n",
    "        \n",
    "        # Use specified model or best model\n",
    "        model_to_use = model_name or self.best_model_name\n",
    "        \n",
    "        if model_to_use not in self.models:\n",
    "            raise ValueError(f\"Model '{model_to_use}' not found\")\n",
    "        \n",
    "        # Prepare data\n",
    "        df_clean, _, _, _ = self._prepare_features(new_data)\n",
    "        print(df_clean.shape)\n",
    "        # Use same feature columns as training\n",
    "        if feature_columns is not None:\n",
    "            available_features = [f for f in feature_columns if f in df_clean.columns]\n",
    "        else:\n",
    "            available_features = [f for f in self.feature_columns if f in df_clean.columns]\n",
    "        print(available_features)\n",
    "        X = df_clean[available_features]\n",
    "        # Make predictions\n",
    "        model = self.models[model_to_use]\n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def save_models(self, save_dir=\"models\", prefix=\"fire_models\"):\n",
    "        \"\"\"Save all trained models\"\"\"\n",
    "        \n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"No trained models to save\")\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "        \n",
    "        # Save individual models\n",
    "        model_paths = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            clean_name = model_name.replace(\" \", \"_\").lower()\n",
    "            model_path = f\"{save_dir}/{prefix}_{clean_name}_{timestamp}.pkl\"\n",
    "            joblib.dump(model, model_path)\n",
    "            model_paths[model_name] = model_path\n",
    "        \n",
    "        # Save predictor state\n",
    "        predictor_state = {\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'best_model_name': self.best_model_name,\n",
    "            'training_results': self.training_results,\n",
    "            'model_paths': model_paths\n",
    "        }\n",
    "        \n",
    "        state_path = f\"{save_dir}/{prefix}_state_{timestamp}.pkl\"\n",
    "        joblib.dump(predictor_state, state_path)\n",
    "        \n",
    "        print(f\"Models saved to {save_dir} with timestamp {timestamp}\")\n",
    "        return timestamp\n",
    "    \n",
    "    def load_models(self, save_dir=\"models\", timestamp=None, prefix=\"fire_models\"):\n",
    "        \"\"\"Load previously saved models\"\"\"\n",
    "        \n",
    "        import glob\n",
    "        \n",
    "        if timestamp is None:\n",
    "            # Find most recent\n",
    "            state_files = glob.glob(f\"{save_dir}/{prefix}_state_*.pkl\")\n",
    "            if not state_files:\n",
    "                raise ValueError(\"No saved models found\")\n",
    "            state_path = max(state_files, key=os.path.getctime)\n",
    "        else:\n",
    "            state_path = f\"{save_dir}/{prefix}_state_{timestamp}.pkl\"\n",
    "        \n",
    "        # Load predictor state\n",
    "        predictor_state = joblib.load(state_path)\n",
    "        \n",
    "        self.feature_columns = predictor_state['feature_columns']\n",
    "        self.best_model_name = predictor_state['best_model_name']\n",
    "        self.training_results = predictor_state['training_results']\n",
    "        \n",
    "        # Load individual models\n",
    "        self.models = {}\n",
    "        for model_name, model_path in predictor_state['model_paths'].items():\n",
    "            self.models[model_name] = joblib.load(model_path)\n",
    "        \n",
    "        self.best_model = self.models[self.best_model_name]\n",
    "        self.is_trained = True\n",
    "        \n",
    "        print(f\"Models loaded successfully\")\n",
    "        print(f\"Available models: {list(self.models.keys())}\")\n",
    "        print(f\"Best model: {self.best_model_name}\")\n",
    "    \n",
    "    def get_model_performance(self):\n",
    "        \"\"\"Get training performance of all models\"\"\"\n",
    "        if not self.is_trained:\n",
    "            return \"No trained models available\"\n",
    "        \n",
    "        return pd.DataFrame(self.training_results).T.sort_values('test_mae')\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Initialize regression models with fire-specific parameters\"\"\"\n",
    "        self.models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge Regression': Ridge(alpha=1.0),\n",
    "            'Lasso Regression': Lasso(alpha=1.0),\n",
    "            'Elastic Net': ElasticNet(alpha=1.0),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42),\n",
    "            'Extra Trees': ExtraTreesRegressor(n_estimators=50, max_depth=10, random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=50, max_depth=4, random_state=42),\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Enhanced hyperparameter grids\n",
    "        self.param_grids = {\n",
    "            'Ridge Regression': {'regressor__alpha': [0.1, 1.0, 10.0, 100.0]},\n",
    "            'Lasso Regression': {'regressor__alpha': [0.1, 1.0, 10.0, 100.0]},\n",
    "            'Elastic Net': {\n",
    "                'regressor__alpha': [0.1, 1.0, 10.0],\n",
    "                'regressor__l1_ratio': [0.1, 0.5, 0.9]\n",
    "            },\n",
    "            'Random Forest': {\n",
    "                'regressor__n_estimators': [50, 100],\n",
    "                'regressor__max_depth': [5, 10],\n",
    "                'regressor__min_samples_split': [5, 10]\n",
    "            },\n",
    "            'Extra Trees': {\n",
    "                'regressor__n_estimators': [50, 100],\n",
    "                'regressor__max_depth': [5, 10], \n",
    "                'regressor__min_samples_split': [5, 10]\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'regressor__n_estimators': [50, 100],\n",
    "                'regressor__learning_rate': [0.1, 0.2],\n",
    "                'regressor__max_depth': [3, 4]\n",
    "            },\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "predictor = FireIncidentResponsePredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438338dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "incidents=pd.read_csv('scripts/incidents_data_for_modeling2.csv')  # Example data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d418119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating workload features using merge approach...\n",
      "Calculating category-specific workload...\n",
      "Calculating zone-specific workload...\n",
      "Workload feature calculation completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "incidents=pd.read_csv('scripts/incidents_data_for_modeling2.csv')  # Example data loading\n",
    "incidents_featurized = engineer_features(incidents)  # Feature engineering on a subset for speed\n",
    "# engineer_features.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77deea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f0b5f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92559, 27)\n",
      "['hour', 'day_of_week', 'month', 'quarter', 'day_of_year', 'is_weekend', 'is_night', 'is_rush_hour', 'is_business_hours', 'is_holiday', 'category_incidents_last_24h', 'system_incidents_last_hour', 'lat', 'lon', 'distance_from_center', 'zone_incidents_last_week', 'zone_incidents_last_month', 'shift', 'season', 'category', 'ZONE_ID', 'incident_type']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2479.38960092, 1983.98320741, 2139.6307051 , ..., 2159.52443304,\n",
       "       2008.03492462, 1953.82326073], shape=(92559,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.models['Random Forest'] = joblib.load('scripts/fire_models_random_forest_20250917.pkl')\n",
    "predictor.is_trained = True\n",
    "_, feature_columns, _, _ = predictor._prepare_features(incidents_featurized)\n",
    "predictor.predict(incidents_featurized, model_name='Random Forest', feature_columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35fd7243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_night</th>\n",
       "      <th>is_rush_hour</th>\n",
       "      <th>is_business_hours</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>category_incidents_last_24h</th>\n",
       "      <th>system_incidents_last_hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>distance_from_center</th>\n",
       "      <th>zone_incidents_last_week</th>\n",
       "      <th>zone_incidents_last_month</th>\n",
       "      <th>shift</th>\n",
       "      <th>season</th>\n",
       "      <th>category</th>\n",
       "      <th>ZONE_ID</th>\n",
       "      <th>incident_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.213787</td>\n",
       "      <td>-86.596595</td>\n",
       "      <td>10.904416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Nine</td>\n",
       "      <td>210.0</td>\n",
       "      <td>Nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.160913</td>\n",
       "      <td>-86.776837</td>\n",
       "      <td>0.292999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Nine</td>\n",
       "      <td>327.0</td>\n",
       "      <td>Nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.044009</td>\n",
       "      <td>-86.628408</td>\n",
       "      <td>11.849202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Nine</td>\n",
       "      <td>222.0</td>\n",
       "      <td>Nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36.151075</td>\n",
       "      <td>-86.761818</td>\n",
       "      <td>1.365001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Five</td>\n",
       "      <td>324.0</td>\n",
       "      <td>Five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.226771</td>\n",
       "      <td>-86.725614</td>\n",
       "      <td>5.417207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Nine</td>\n",
       "      <td>174.0</td>\n",
       "      <td>Nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92554</th>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.104282</td>\n",
       "      <td>-86.742205</td>\n",
       "      <td>4.596416</td>\n",
       "      <td>9.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Nine</td>\n",
       "      <td>111.0</td>\n",
       "      <td>Nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92555</th>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.158816</td>\n",
       "      <td>-86.798171</td>\n",
       "      <td>0.962613</td>\n",
       "      <td>12.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Nine</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92556</th>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.199841</td>\n",
       "      <td>-86.712963</td>\n",
       "      <td>4.608699</td>\n",
       "      <td>11.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Nine</td>\n",
       "      <td>69.0</td>\n",
       "      <td>Nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92557</th>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.125805</td>\n",
       "      <td>-86.765879</td>\n",
       "      <td>2.696055</td>\n",
       "      <td>9.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Nine</td>\n",
       "      <td>318.0</td>\n",
       "      <td>Nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92558</th>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.082343</td>\n",
       "      <td>-86.713397</td>\n",
       "      <td>6.732078</td>\n",
       "      <td>63.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>Night</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Nine</td>\n",
       "      <td>303.0</td>\n",
       "      <td>Nine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92559 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hour  day_of_week  month  quarter  day_of_year  is_weekend  is_night  \\\n",
       "0         0            0      1        1            1           0         1   \n",
       "1         0            0      1        1            1           0         1   \n",
       "2         0            0      1        1            1           0         1   \n",
       "3         0            0      1        1            1           0         1   \n",
       "4         0            0      1        1            1           0         1   \n",
       "...     ...          ...    ...      ...          ...         ...       ...   \n",
       "92554    23            6      6        2          173           1         1   \n",
       "92555    23            6      6        2          173           1         1   \n",
       "92556    23            6      6        2          173           1         1   \n",
       "92557    23            6      6        2          173           1         1   \n",
       "92558    23            6      6        2          173           1         1   \n",
       "\n",
       "       is_rush_hour  is_business_hours  is_holiday  \\\n",
       "0                 0                  0           0   \n",
       "1                 0                  0           0   \n",
       "2                 0                  0           0   \n",
       "3                 0                  0           0   \n",
       "4                 0                  0           0   \n",
       "...             ...                ...         ...   \n",
       "92554             0                  0           0   \n",
       "92555             0                  0           0   \n",
       "92556             0                  0           0   \n",
       "92557             0                  0           0   \n",
       "92558             0                  0           0   \n",
       "\n",
       "       category_incidents_last_24h  system_incidents_last_hour        lat  \\\n",
       "0                              0.0                         0.0  36.213787   \n",
       "1                              1.0                         1.0  36.160913   \n",
       "2                              2.0                         2.0  36.044009   \n",
       "3                              0.0                         3.0  36.151075   \n",
       "4                              3.0                         4.0  36.226771   \n",
       "...                            ...                         ...        ...   \n",
       "92554                        343.0                        15.0  36.104282   \n",
       "92555                        344.0                        15.0  36.158816   \n",
       "92556                        344.0                        16.0  36.199841   \n",
       "92557                        344.0                        17.0  36.125805   \n",
       "92558                        345.0                        17.0  36.082343   \n",
       "\n",
       "             lon  distance_from_center  zone_incidents_last_week  \\\n",
       "0     -86.596595             10.904416                       0.0   \n",
       "1     -86.776837              0.292999                       0.0   \n",
       "2     -86.628408             11.849202                       0.0   \n",
       "3     -86.761818              1.365001                       0.0   \n",
       "4     -86.725614              5.417207                       0.0   \n",
       "...          ...                   ...                       ...   \n",
       "92554 -86.742205              4.596416                       9.0   \n",
       "92555 -86.798171              0.962613                      12.0   \n",
       "92556 -86.712963              4.608699                      11.0   \n",
       "92557 -86.765879              2.696055                       9.0   \n",
       "92558 -86.713397              6.732078                      63.0   \n",
       "\n",
       "       zone_incidents_last_month  shift  season category  ZONE_ID  \\\n",
       "0                            0.0  Night  Winter     Nine    210.0   \n",
       "1                            0.0  Night  Winter     Nine    327.0   \n",
       "2                            0.0  Night  Winter     Nine    222.0   \n",
       "3                            0.0  Night  Winter     Five    324.0   \n",
       "4                            0.0  Night  Winter     Nine    174.0   \n",
       "...                          ...    ...     ...      ...      ...   \n",
       "92554                       31.0  Night  Summer     Nine    111.0   \n",
       "92555                       27.0  Night  Summer     Nine     88.0   \n",
       "92556                       45.0  Night  Summer     Nine     69.0   \n",
       "92557                       37.0  Night  Summer     Nine    318.0   \n",
       "92558                      217.0  Night  Summer     Nine    303.0   \n",
       "\n",
       "      incident_type  \n",
       "0              Nine  \n",
       "1              Nine  \n",
       "2              Nine  \n",
       "3              Five  \n",
       "4              Nine  \n",
       "...             ...  \n",
       "92554          Nine  \n",
       "92555          Nine  \n",
       "92556          Nine  \n",
       "92557          Nine  \n",
       "92558          Nine  \n",
       "\n",
       "[92559 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c8c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92559 entries, 0 to 92558\n",
      "Data columns (total 22 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   hour                         92559 non-null  int32  \n",
      " 1   day_of_week                  92559 non-null  int32  \n",
      " 2   month                        92559 non-null  int32  \n",
      " 3   quarter                      92559 non-null  int32  \n",
      " 4   day_of_year                  92559 non-null  int32  \n",
      " 5   is_weekend                   92559 non-null  int64  \n",
      " 6   is_night                     92559 non-null  int64  \n",
      " 7   is_rush_hour                 92559 non-null  int64  \n",
      " 8   is_business_hours            92559 non-null  int64  \n",
      " 9   is_holiday                   92559 non-null  int64  \n",
      " 10  category_incidents_last_24h  92559 non-null  float64\n",
      " 11  system_incidents_last_hour   92559 non-null  float64\n",
      " 12  lat                          92559 non-null  float64\n",
      " 13  lon                          92559 non-null  float64\n",
      " 14  distance_from_center         92559 non-null  float64\n",
      " 15  zone_incidents_last_week     92559 non-null  float64\n",
      " 16  zone_incidents_last_month    92559 non-null  float64\n",
      " 17  shift                        92559 non-null  object \n",
      " 18  season                       92559 non-null  object \n",
      " 19  category                     92559 non-null  object \n",
      " 20  ZONE_ID                      92554 non-null  float64\n",
      " 21  incident_type                92559 non-null  object \n",
      "dtypes: float64(8), int32(5), int64(5), object(4)\n",
      "memory usage: 13.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clean[feature_columns].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56d4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_clean, feature_columns, available_numerical, available_categorical = predictor._prepare_features(incidents_featurized)\n",
    "# available_features = [f for f in predictor.feature_columns if f in df_clean.columns]\n",
    "# X = df_clean[available_features]\n",
    "# available_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a401b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.predict(incidents_featurized, model_name='Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37803d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "venv312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
